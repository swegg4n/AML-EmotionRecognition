{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader, Dataset\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('running on:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 48\n",
    "\n",
    "CLASSES = ['neutral', 'happy', 'surprised', 'sad', 'angry'] #, 'disgusted', 'afraid'\n",
    "NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 35887\n",
      "after removing images with unknown/unsure classification: 26811\n"
     ]
    }
   ],
   "source": [
    "df_fer = pd.read_csv('../data/fer2013.csv').iloc[:, 1:]\n",
    "df_ferplus = pd.read_csv('../data/ferplus2013.csv').iloc[:, [2, 3, 4, 5, 6]]\n",
    "df = df_fer.join(df_ferplus)\n",
    "# df.head(15)\n",
    "\n",
    "df['emotion'] = df.iloc[:, 2:].idxmax(axis=1).tolist()\n",
    "df = df.replace(dict(zip(pd.Series(CLASSES),pd.Series(CLASSES).index)))\n",
    "# df.head(15)\n",
    "\n",
    "print('number of images:', df.shape[0])\n",
    "df = df[(df.iloc[:, 2:-1].max(axis=1) > 5)]\n",
    "print('after removing images with unknown/unsure classification:', df.shape[0])\n",
    "# df.head(15)\n",
    "\n",
    "df = df.iloc[:, [0, 1, -1]]\n",
    "# df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.545), (0.245)),\n",
    "])\n",
    "\n",
    "images_np = np.array(df['pixels'])\n",
    "label_np = np.array(df['emotion'])\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    b = bytes(int(p) for p in images_np[i].split())\n",
    "    img = Image.frombuffer('L', (IMG_SIZE, IMG_SIZE), b)\n",
    "    images.append(transform(img))\n",
    "    labels.append(label_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mclass              #images\u001b[0m\n",
      "neutral               9494\n",
      "happy                 8802\n",
      "surprised             3461\n",
      "sad                   2958\n",
      "angry                 2096\n"
     ]
    }
   ],
   "source": [
    "label_counts = [0]*NUM_CLASSES\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label_counts[labels[i]] += 1\n",
    "\n",
    "print('\\033[4m{: <15} {: >10}\\033[0m'.format('class', '#images'))\n",
    "for i in range(NUM_CLASSES):\n",
    "    print('{: <15} {: >10}'.format(CLASSES[i], label_counts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 18767 (70.0%)\n",
      "test length: 8044 (30.0%)\n"
     ]
    }
   ],
   "source": [
    "train_split_pct = 0.7\n",
    "train_len = int(len(labels) * train_split_pct)\n",
    "\n",
    "train_images = images[:train_len]\n",
    "train_labels = labels[:train_len]\n",
    "test_images = images[train_len:]\n",
    "test_labels = labels[train_len:]\n",
    "\n",
    "print(f'train length: {len(train_labels)} ({len(train_labels)*100/len(labels):.1f}%)')\n",
    "print(f'test length: {len(test_labels)} ({len(test_labels)*100/len(labels):.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_loader():\n",
      "  class weights: [ 2.823  3.008  7.849  9.044 13.263] \n",
      "\n",
      "get_loader():\n",
      "  class weights: [ 2.825  3.139  7.518  9.11  11.812] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels):      \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_images, train_labels)\n",
    "test_dataset = CustomDataset(test_images, test_labels)\n",
    "\n",
    "\n",
    "def get_loader(dataset):\n",
    "    print('get_loader():')\n",
    "\n",
    "    label_counts = [0]*NUM_CLASSES\n",
    "    for i in range(len(dataset.labels)):\n",
    "        label_counts[dataset.labels[i]] += 1\n",
    "\n",
    "    class_weights = pow(np.array(label_counts) / sum(label_counts), -1)\n",
    "    print('  class weights:', class_weights.round(3), '\\n')\n",
    "\n",
    "    sample_weights = [0]*len(dataset)\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        sample_weights[idx] = class_weights[label]\n",
    "\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    return DataLoader(dataset, BATCH_SIZE, sampler=sampler, shuffle=False)\n",
    "\n",
    "\n",
    "train_loader = get_loader(train_dataset)\n",
    "test_loader = get_loader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size during training, remember to change the global one as well\n",
    "batch_size = 128\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'happy', 'surprised', 'sad', 'angry']\n"
     ]
    }
   ],
   "source": [
    "# Function for displaying predictions for a few images, in this case 6.\n",
    "#print(test_set[0])\n",
    "\n",
    "class_names = CLASSES\n",
    "print(class_names)\n",
    "\n",
    "def visualize_expression_model(model, num_images=4):\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                plt.imshow(inputs.cpu().data[j].permute(1, 2, 0), cmap=\"gray\")\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_expression_model(model, data, criterion, optimizer):\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    model.train()   # Set model to train mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in data:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # zero the parameter gradients, and use backpropagatation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "    print('train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_expression_model(model, data, criterion):\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in data:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(test_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(test_dataset)\n",
    "\n",
    "    print('test Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a recreation of the AlexNet model\n",
    "\n",
    "class Facial_Expression_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Facial_Expression_Network, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, IMG_SIZE, kernel_size=(3,3), stride=(1, 1), padding=(2, 2)), \n",
    "            nn.ReLU(inplace=True), # Rectified Linear Unit activation function\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), # pooling layer for reducing dimensions\n",
    "            nn.Conv2d(IMG_SIZE, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(6, 6)) # Applies a 2D adaptive average pooling over an input composed of several input planes.\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=False), # Dropout layer for setting 50% of the activations to 0, fording the network to not rely on any 1 node\n",
    "            nn.Linear(in_features=9216, out_features=4096, bias=True), # Linear layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=4096, out_features=5, bias=True),\n",
    "        ) # ---- Change out_features if change in amount of classes ----\n",
    "\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1) # last activation function for the network, normalizing the output\n",
    "        self.softmax_result = 0 \n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # The data needs to be flattened after the AdaptiveAvgPool2d as its output is H x W\n",
    "        # This is because the classifier's first layer is a Linear layer\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.classifier(x) \n",
    "        self.softmax_result = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_model = Facial_Expression_Network().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(expression_model.parameters(), lr=lr, momentum = 0.9) # So far, only 31.1% acc\n",
    "#optimizer = torch.optim.Adam(expression_model.parameters(), lr=lr)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 1 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "train Loss: 0.0126 Acc: 0.1969\n",
      "test Loss: 0.0126 Acc: 0.1965\n",
      "Epoch 2/30\n",
      "\n",
      "train Loss: 0.0126 Acc: 0.2034\n",
      "test Loss: 0.0126 Acc: 0.2030\n",
      "Epoch 3/30\n",
      "\n",
      "train Loss: 0.0126 Acc: 0.2066\n",
      "test Loss: 0.0126 Acc: 0.2075\n",
      "Epoch 4/30\n",
      "\n",
      "train Loss: 0.0126 Acc: 0.2101\n",
      "test Loss: 0.0126 Acc: 0.2015\n",
      "Epoch 5/30\n",
      "\n",
      "train Loss: 0.0126 Acc: 0.2082\n",
      "test Loss: 0.0126 Acc: 0.1988\n",
      "Epoch 6/30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, (num_epochs+1)):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "    print()\n",
    "    train_expression_model(expression_model, train_loader, loss_fn, optimizer)\n",
    "    test_expression_model(expression_model, test_loader, loss_fn)\n",
    "    scheduler.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_expression_model(expression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of AlexNet Model\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train = [0]*NUM_CLASSES\n",
    "samples_test = [0]*NUM_CLASSES\n",
    "\n",
    "for idx, (images, labels) in enumerate(train_loader):\n",
    "    for i in range(len(images)):\n",
    "        samples_train[labels[i].item()] += 1\n",
    "\n",
    "for idx, (images, labels) in enumerate(test_loader):\n",
    "    for i in range(len(images)):\n",
    "        samples_test[labels[i].item()] += 1\n",
    "\n",
    "\n",
    "print('\\033[4m{: <15} {: >15} {: >20}\\033[0m'.format('class', '#samples (train)', '#samples (test)'))\n",
    "for i in range(NUM_CLASSES):\n",
    "    print('{: <15} {: >15} {: >20}'.format(CLASSES[i], samples_train[i], samples_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(dataset, title='', num_images=(3,6), rand=True):\n",
    "\n",
    "    plt.figure(figsize=(num_images[1]*1.5, num_images[0]*2)); \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i in range(num_images[0]):\n",
    "        for j in range(num_images[1]):\n",
    "            \n",
    "            c = num_images[1]*i+(j+1)\n",
    "\n",
    "            if not rand:\n",
    "                idx = c\n",
    "            else:\n",
    "                idx = random.randint(0, len(dataset)-1)\n",
    "\n",
    "            plt.subplot(num_images[0], num_images[1], c); plt.axis('off'); plt.title(CLASSES[dataset[idx][1]])\n",
    "            plt.imshow(dataset[idx][0].permute(1,2,0), cmap='gray')\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "imshow(train_dataset, 'Example images from the train dataset')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "462459bc3e617147c8cffc282d893ee434b18ff4bde70f08c233dfc73bb28fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
